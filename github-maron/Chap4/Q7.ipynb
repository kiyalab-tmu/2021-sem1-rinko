{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROemRGQLIPZ8",
    "outputId": "556188b0-c291-4afd-c19f-6e3982db8b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#AlexNet\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 11, stride = 4)\n",
    "        self.pool = nn.MaxPool2d(3, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 256, kernel_size = 5, padding = 2)\n",
    "        self.conv3= nn.Conv2d(256, 384, kernel_size = 3, padding = 1)\n",
    "        self.conv4= nn.Conv2d(384, 384, kernel_size = 3, padding = 1)\n",
    "        self.conv5= nn.Conv2d(384, 256, kernel_size = 3, padding = 1)\n",
    "        self.fc1 = nn.Linear(256*5*5, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = x.view(-1, 256*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "batch_size = 256\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),transforms.ToTensor(), transforms.Normalize((0.5),(0.5))])\n",
    "\n",
    "fashion_mnist_trainval = FashionMNIST(\"FashionMNIST\", train=True, download=True, transform=transform)\n",
    "fashion_mnist_test = FashionMNIST(\"FashionMNIST\", train=False, download=True, transform=transform)\n",
    "\n",
    "n_samples = len(fashion_mnist_trainval) \n",
    "train_size = int(len(fashion_mnist_trainval) * 0.8) \n",
    "val_size = n_samples - train_size \n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(fashion_mnist_trainval, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(fashion_mnist_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dEjVvCXJwFcs",
    "outputId": "2220eda4-9edd-4ec7-8c59-3a6efbba4c29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, train_loss : 2.3024, val_loss : 2.3021\n",
      "epoch : 1, train_loss : 2.3021, val_loss : 2.3017\n",
      "epoch : 2, train_loss : 2.3017, val_loss : 2.3013\n",
      "epoch : 3, train_loss : 2.3013, val_loss : 2.3010\n",
      "epoch : 4, train_loss : 2.3009, val_loss : 2.3004\n",
      "epoch : 5, train_loss : 2.3003, val_loss : 2.2998\n",
      "epoch : 6, train_loss : 2.2997, val_loss : 2.2991\n",
      "epoch : 7, train_loss : 2.2988, val_loss : 2.2981\n",
      "epoch : 8, train_loss : 2.2977, val_loss : 2.2968\n",
      "epoch : 9, train_loss : 2.2963, val_loss : 2.2950\n",
      "epoch : 10, train_loss : 2.2941, val_loss : 2.2924\n",
      "epoch : 11, train_loss : 2.2907, val_loss : 2.2878\n",
      "epoch : 12, train_loss : 2.2846, val_loss : 2.2786\n",
      "epoch : 13, train_loss : 2.2693, val_loss : 2.2498\n",
      "epoch : 14, train_loss : 2.2061, val_loss : 2.1026\n",
      "epoch : 15, train_loss : 1.6853, val_loss : 1.2407\n",
      "epoch : 16, train_loss : 1.0832, val_loss : 0.9639\n",
      "epoch : 17, train_loss : 0.9006, val_loss : 0.8475\n",
      "epoch : 18, train_loss : 0.8236, val_loss : 0.7873\n",
      "epoch : 19, train_loss : 0.7753, val_loss : 0.7576\n",
      "epoch : 20, train_loss : 0.7480, val_loss : 0.7315\n",
      "epoch : 21, train_loss : 0.7226, val_loss : 0.7106\n",
      "epoch : 22, train_loss : 0.7014, val_loss : 0.6909\n",
      "epoch : 23, train_loss : 0.6871, val_loss : 0.6750\n",
      "epoch : 24, train_loss : 0.6717, val_loss : 0.6608\n",
      "epoch : 25, train_loss : 0.6587, val_loss : 0.6508\n",
      "epoch : 26, train_loss : 0.6497, val_loss : 0.6413\n",
      "epoch : 27, train_loss : 0.6377, val_loss : 0.6307\n",
      "epoch : 28, train_loss : 0.6261, val_loss : 0.6168\n",
      "epoch : 29, train_loss : 0.6163, val_loss : 0.6126\n",
      "epoch : 30, train_loss : 0.6055, val_loss : 0.6032\n",
      "epoch : 31, train_loss : 0.5995, val_loss : 0.5956\n",
      "epoch : 32, train_loss : 0.5894, val_loss : 0.5832\n",
      "epoch : 33, train_loss : 0.5783, val_loss : 0.5748\n",
      "epoch : 34, train_loss : 0.5716, val_loss : 0.5724\n",
      "epoch : 35, train_loss : 0.5671, val_loss : 0.5609\n",
      "epoch : 36, train_loss : 0.5577, val_loss : 0.5558\n",
      "epoch : 37, train_loss : 0.5520, val_loss : 0.5456\n",
      "epoch : 38, train_loss : 0.5457, val_loss : 0.5394\n",
      "epoch : 39, train_loss : 0.5374, val_loss : 0.5311\n",
      "epoch : 40, train_loss : 0.5285, val_loss : 0.5258\n",
      "epoch : 41, train_loss : 0.5245, val_loss : 0.5217\n",
      "count: 1\n",
      "epoch : 42, train_loss : 0.5175, val_loss : 0.5220\n",
      "epoch : 43, train_loss : 0.5111, val_loss : 0.5112\n",
      "epoch : 44, train_loss : 0.5064, val_loss : 0.5042\n",
      "epoch : 45, train_loss : 0.5009, val_loss : 0.4991\n",
      "epoch : 46, train_loss : 0.4963, val_loss : 0.4950\n",
      "epoch : 47, train_loss : 0.4913, val_loss : 0.4904\n",
      "count: 1\n",
      "epoch : 48, train_loss : 0.4879, val_loss : 0.4906\n",
      "epoch : 49, train_loss : 0.4837, val_loss : 0.4817\n",
      "epoch : 50, train_loss : 0.4801, val_loss : 0.4773\n",
      "epoch : 51, train_loss : 0.4757, val_loss : 0.4735\n",
      "epoch : 52, train_loss : 0.4689, val_loss : 0.4723\n",
      "epoch : 53, train_loss : 0.4657, val_loss : 0.4627\n",
      "count: 1\n",
      "epoch : 54, train_loss : 0.4634, val_loss : 0.4636\n",
      "epoch : 55, train_loss : 0.4619, val_loss : 0.4577\n",
      "epoch : 56, train_loss : 0.4552, val_loss : 0.4561\n",
      "epoch : 57, train_loss : 0.4524, val_loss : 0.4533\n",
      "epoch : 58, train_loss : 0.4479, val_loss : 0.4457\n",
      "count: 1\n",
      "epoch : 59, train_loss : 0.4462, val_loss : 0.4477\n",
      "epoch : 60, train_loss : 0.4430, val_loss : 0.4392\n",
      "epoch : 61, train_loss : 0.4386, val_loss : 0.4355\n",
      "count: 1\n",
      "epoch : 62, train_loss : 0.4393, val_loss : 0.4366\n",
      "epoch : 63, train_loss : 0.4327, val_loss : 0.4295\n",
      "count: 1\n",
      "epoch : 64, train_loss : 0.4323, val_loss : 0.4297\n",
      "epoch : 65, train_loss : 0.4293, val_loss : 0.4254\n",
      "epoch : 66, train_loss : 0.4269, val_loss : 0.4233\n",
      "epoch : 67, train_loss : 0.4224, val_loss : 0.4203\n",
      "epoch : 68, train_loss : 0.4216, val_loss : 0.4182\n",
      "epoch : 69, train_loss : 0.4186, val_loss : 0.4173\n",
      "epoch : 70, train_loss : 0.4164, val_loss : 0.4163\n",
      "epoch : 71, train_loss : 0.4146, val_loss : 0.4152\n",
      "epoch : 72, train_loss : 0.4128, val_loss : 0.4109\n",
      "count: 1\n",
      "epoch : 73, train_loss : 0.4104, val_loss : 0.4119\n",
      "epoch : 74, train_loss : 0.4068, val_loss : 0.4039\n",
      "epoch : 75, train_loss : 0.4042, val_loss : 0.4034\n",
      "epoch : 76, train_loss : 0.4034, val_loss : 0.3986\n",
      "count: 1\n",
      "epoch : 77, train_loss : 0.4009, val_loss : 0.4003\n",
      "epoch : 78, train_loss : 0.4004, val_loss : 0.3955\n",
      "epoch : 79, train_loss : 0.3973, val_loss : 0.3931\n",
      "epoch : 80, train_loss : 0.3945, val_loss : 0.3927\n",
      "epoch : 81, train_loss : 0.3934, val_loss : 0.3916\n",
      "epoch : 82, train_loss : 0.3891, val_loss : 0.3867\n",
      "epoch : 83, train_loss : 0.3894, val_loss : 0.3841\n",
      "epoch : 84, train_loss : 0.3901, val_loss : 0.3819\n",
      "epoch : 85, train_loss : 0.3872, val_loss : 0.3813\n",
      "epoch : 86, train_loss : 0.3837, val_loss : 0.3797\n",
      "count: 1\n",
      "epoch : 87, train_loss : 0.3820, val_loss : 0.3832\n",
      "count: 2\n",
      "epoch : 88, train_loss : 0.3809, val_loss : 0.3812\n",
      "epoch : 89, train_loss : 0.3787, val_loss : 0.3740\n",
      "count: 1\n",
      "epoch : 90, train_loss : 0.3785, val_loss : 0.3745\n",
      "epoch : 91, train_loss : 0.3755, val_loss : 0.3717\n",
      "epoch : 92, train_loss : 0.3743, val_loss : 0.3704\n",
      "count: 1\n",
      "epoch : 93, train_loss : 0.3725, val_loss : 0.3766\n",
      "epoch : 94, train_loss : 0.3715, val_loss : 0.3693\n",
      "epoch : 95, train_loss : 0.3715, val_loss : 0.3692\n",
      "epoch : 96, train_loss : 0.3682, val_loss : 0.3653\n",
      "epoch : 97, train_loss : 0.3667, val_loss : 0.3630\n",
      "count: 1\n",
      "epoch : 98, train_loss : 0.3639, val_loss : 0.3658\n",
      "count: 2\n",
      "epoch : 99, train_loss : 0.3657, val_loss : 0.3637\n",
      "epoch : 100, train_loss : 0.3639, val_loss : 0.3616\n",
      "epoch : 101, train_loss : 0.3605, val_loss : 0.3552\n",
      "epoch : 102, train_loss : 0.3597, val_loss : 0.3547\n",
      "count: 1\n",
      "epoch : 103, train_loss : 0.3589, val_loss : 0.3572\n",
      "count: 2\n",
      "epoch : 104, train_loss : 0.3559, val_loss : 0.3549\n",
      "count: 3\n",
      "epoch : 105, train_loss : 0.3544, val_loss : 0.3564\n",
      "epoch : 106, train_loss : 0.3577, val_loss : 0.3512\n",
      "count: 1\n",
      "epoch : 107, train_loss : 0.3523, val_loss : 0.3532\n",
      "count: 2\n",
      "epoch : 108, train_loss : 0.3537, val_loss : 0.3526\n",
      "epoch : 109, train_loss : 0.3521, val_loss : 0.3480\n",
      "count: 1\n",
      "epoch : 110, train_loss : 0.3498, val_loss : 0.3520\n",
      "epoch : 111, train_loss : 0.3480, val_loss : 0.3462\n",
      "epoch : 112, train_loss : 0.3477, val_loss : 0.3432\n",
      "epoch : 113, train_loss : 0.3462, val_loss : 0.3409\n",
      "count: 1\n",
      "epoch : 114, train_loss : 0.3447, val_loss : 0.3412\n",
      "epoch : 115, train_loss : 0.3437, val_loss : 0.3389\n",
      "count: 1\n",
      "epoch : 116, train_loss : 0.3419, val_loss : 0.3403\n",
      "epoch : 117, train_loss : 0.3432, val_loss : 0.3365\n",
      "count: 1\n",
      "epoch : 118, train_loss : 0.3406, val_loss : 0.3433\n",
      "epoch : 119, train_loss : 0.3393, val_loss : 0.3344\n",
      "count: 1\n",
      "epoch : 120, train_loss : 0.3377, val_loss : 0.3388\n",
      "count: 2\n",
      "epoch : 121, train_loss : 0.3389, val_loss : 0.3393\n",
      "epoch : 122, train_loss : 0.3371, val_loss : 0.3297\n",
      "count: 1\n",
      "epoch : 123, train_loss : 0.3374, val_loss : 0.3311\n",
      "count: 2\n",
      "epoch : 124, train_loss : 0.3347, val_loss : 0.3315\n",
      "count: 3\n",
      "epoch : 125, train_loss : 0.3331, val_loss : 0.3300\n",
      "epoch : 126, train_loss : 0.3323, val_loss : 0.3288\n",
      "count: 1\n",
      "epoch : 127, train_loss : 0.3334, val_loss : 0.3291\n",
      "epoch : 128, train_loss : 0.3310, val_loss : 0.3244\n",
      "count: 1\n",
      "epoch : 129, train_loss : 0.3304, val_loss : 0.3273\n",
      "count: 2\n",
      "epoch : 130, train_loss : 0.3271, val_loss : 0.3244\n",
      "epoch : 131, train_loss : 0.3278, val_loss : 0.3241\n",
      "epoch : 132, train_loss : 0.3256, val_loss : 0.3202\n",
      "count: 1\n",
      "epoch : 133, train_loss : 0.3234, val_loss : 0.3247\n",
      "count: 2\n",
      "epoch : 134, train_loss : 0.3267, val_loss : 0.3299\n",
      "count: 3\n",
      "epoch : 135, train_loss : 0.3233, val_loss : 0.3216\n",
      "count: 4\n",
      "epoch : 136, train_loss : 0.3226, val_loss : 0.3222\n",
      "epoch : 137, train_loss : 0.3201, val_loss : 0.3171\n",
      "count: 1\n",
      "epoch : 138, train_loss : 0.3208, val_loss : 0.3174\n",
      "count: 2\n",
      "epoch : 139, train_loss : 0.3201, val_loss : 0.3181\n",
      "epoch : 140, train_loss : 0.3189, val_loss : 0.3169\n",
      "count: 1\n",
      "epoch : 141, train_loss : 0.3190, val_loss : 0.3183\n",
      "count: 2\n",
      "epoch : 142, train_loss : 0.3174, val_loss : 0.3184\n",
      "epoch : 143, train_loss : 0.3180, val_loss : 0.3149\n",
      "epoch : 144, train_loss : 0.3152, val_loss : 0.3112\n",
      "count: 1\n",
      "epoch : 145, train_loss : 0.3153, val_loss : 0.3144\n",
      "epoch : 146, train_loss : 0.3149, val_loss : 0.3087\n",
      "count: 1\n",
      "epoch : 147, train_loss : 0.3135, val_loss : 0.3110\n",
      "epoch : 148, train_loss : 0.3112, val_loss : 0.3055\n",
      "count: 1\n",
      "epoch : 149, train_loss : 0.3118, val_loss : 0.3093\n",
      "count: 2\n",
      "epoch : 150, train_loss : 0.3122, val_loss : 0.3084\n",
      "count: 3\n",
      "epoch : 151, train_loss : 0.3095, val_loss : 0.3082\n",
      "count: 4\n",
      "epoch : 152, train_loss : 0.3105, val_loss : 0.3060\n",
      "epoch : 153, train_loss : 0.3101, val_loss : 0.3055\n",
      "epoch : 154, train_loss : 0.3077, val_loss : 0.3054\n",
      "epoch : 155, train_loss : 0.3072, val_loss : 0.3044\n",
      "epoch : 156, train_loss : 0.3047, val_loss : 0.2998\n",
      "count: 1\n",
      "epoch : 157, train_loss : 0.3061, val_loss : 0.3029\n",
      "count: 2\n",
      "epoch : 158, train_loss : 0.3039, val_loss : 0.3013\n",
      "epoch : 159, train_loss : 0.3041, val_loss : 0.2984\n",
      "count: 1\n",
      "epoch : 160, train_loss : 0.3045, val_loss : 0.2985\n",
      "count: 2\n",
      "epoch : 161, train_loss : 0.3025, val_loss : 0.2995\n",
      "epoch : 162, train_loss : 0.3024, val_loss : 0.2978\n",
      "epoch : 163, train_loss : 0.3031, val_loss : 0.2932\n",
      "count: 1\n",
      "epoch : 164, train_loss : 0.2992, val_loss : 0.2958\n",
      "count: 2\n",
      "epoch : 165, train_loss : 0.3010, val_loss : 0.2999\n",
      "count: 3\n",
      "epoch : 166, train_loss : 0.3010, val_loss : 0.2950\n",
      "epoch : 167, train_loss : 0.2954, val_loss : 0.2898\n",
      "count: 1\n",
      "epoch : 168, train_loss : 0.2968, val_loss : 0.2944\n",
      "count: 2\n",
      "epoch : 169, train_loss : 0.2978, val_loss : 0.2964\n",
      "count: 3\n",
      "epoch : 170, train_loss : 0.2957, val_loss : 0.2933\n",
      "count: 4\n",
      "epoch : 171, train_loss : 0.2946, val_loss : 0.2902\n",
      "count: 5\n",
      "early stopping\n"
     ]
    }
   ],
   "source": [
    "net = AlexNet()\n",
    "net.to(device)\n",
    "\n",
    "best_score = 100.0\n",
    "count = 0\n",
    "stop = 5\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.002)\n",
    "epoch_num = 200\n",
    "running_loss = np.zeros(epoch_num)\n",
    "val_running_loss = np.zeros(epoch_num)\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad() \n",
    "        outputs = net(inputs) \n",
    "        loss = criterion(outputs, labels) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "        running_loss[epoch] += loss.item()\n",
    "        \n",
    "    for i, data in enumerate(val_loader, 0):    \n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad() \n",
    "        outputs = net(inputs) \n",
    "        val_loss = criterion(outputs, labels) \n",
    "        val_loss.backward() \n",
    "        optimizer.step() \n",
    "        val_running_loss[epoch] += val_loss.item()\n",
    "        \n",
    "    running_loss[epoch] /= len(train_loader)\n",
    "    val_running_loss[epoch] /= len(val_loader)\n",
    "\n",
    "    if val_running_loss[epoch] > best_score:\n",
    "        count += 1\n",
    "        print(\"count:\", count)\n",
    "\n",
    "    else:\n",
    "        count = 0\n",
    "        best_score = val_running_loss[epoch]\n",
    "\n",
    "    if count >= stop:\n",
    "        print(\"early stopping\")\n",
    "        break\n",
    "  \n",
    "\n",
    "    print(\"epoch : %d, train_loss : %.4lf, val_loss : %.4lf\" % (epoch, running_loss[epoch],val_running_loss[epoch]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "Fl6zWneUwLha",
    "outputId": "3eddda16-aad6-462f-8fe2-f30df756e15c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f30917d54d0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(running_loss[0:epoch+1])\n",
    "plt.plot(val_running_loss[0:epoch+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dMm3xCb9wMgI",
    "outputId": "6941e00d-d6af-462f-a873-41c73ce764fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.8891875\n",
      "test_acc: 0.875\n"
     ]
    }
   ],
   "source": [
    "train_acc = 0.0\n",
    "correct = 0.0\n",
    "count = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, pred_label = torch.max(outputs.data, 1)\n",
    "        for j in range(len(pred_label)):\n",
    "            if pred_label[j].int() == labels[j]:\n",
    "            correct += 1\n",
    "        count += 1\n",
    "\n",
    "train_acc = correct/count\n",
    "print(\"train_acc:\",train_acc)\n",
    "\n",
    "\n",
    "\n",
    "test_acc = 0.0\n",
    "correct = 0.0\n",
    "count = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, pred_label = torch.max(outputs.data, 1)\n",
    "    \n",
    "    for j in range(len(pred_label)):\n",
    "        if pred_label[j].int() == labels[j]:\n",
    "            correct += 1  \n",
    "        count += 1\n",
    "\n",
    "test_acc = correct/count\n",
    "print(\"test_acc:\",test_acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Q7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
