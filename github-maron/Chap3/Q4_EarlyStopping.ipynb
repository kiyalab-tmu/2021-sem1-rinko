{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "batch_size = 256\n",
    "classes = 10\n",
    "\n",
    "def Relu(y):\n",
    "    return torch.maximum(torch.tensor(0), y)\n",
    "\n",
    "def softmax(y):\n",
    "    for i in range(len(y)):\n",
    "        y[i] = torch.exp(y[i])/torch.sum(torch.exp(y[i]))\n",
    "    return y  \n",
    "        \n",
    "def cross_entropy_loss(p, q):\n",
    "    tmp = torch.zeros((len(q), classes))\n",
    "    for i in range((len(q))):\n",
    "        tmp[i][p[i].item()] += 1.0\n",
    "    \n",
    "    ans = torch.zeros((len(q)))\n",
    "    \n",
    "    for i in range((len(q))):\n",
    "        ans[i] = torch.sum(-(tmp[i]*torch.log(q[i])))    \n",
    "    return torch.mean(ans)\n",
    "\n",
    "def whichclass(pred_y):\n",
    "    _, label = torch.max(pred_y, 1)\n",
    "    return label        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, train_loss : 1.0416, val_loss : 0.157547\n",
      "epoch : 1, train_loss : 0.5438, val_loss : 0.122510\n",
      "epoch : 2, train_loss : 0.4628, val_loss : 0.108854\n",
      "epoch : 3, train_loss : 0.4216, val_loss : 0.100766\n",
      "epoch : 4, train_loss : 0.3946, val_loss : 0.095892\n",
      "epoch : 5, train_loss : 0.3760, val_loss : 0.091877\n",
      "epoch : 6, train_loss : 0.3622, val_loss : 0.088390\n",
      "epoch : 7, train_loss : 0.3513, val_loss : 0.086524\n",
      "epoch : 8, train_loss : 0.3422, val_loss : 0.084825\n",
      "epoch : 9, train_loss : 0.3344, val_loss : 0.083425\n",
      "epoch : 10, train_loss : 0.3276, val_loss : 0.081439\n",
      "epoch : 11, train_loss : 0.3221, val_loss : 0.080694\n",
      "count: 1\n",
      "epoch : 12, train_loss : 0.3170, val_loss : 0.080935\n",
      "epoch : 13, train_loss : 0.3123, val_loss : 0.078140\n",
      "epoch : 14, train_loss : 0.3087, val_loss : 0.078116\n",
      "epoch : 15, train_loss : 0.3039, val_loss : 0.076304\n",
      "epoch : 16, train_loss : 0.3005, val_loss : 0.075577\n",
      "count: 1\n",
      "epoch : 17, train_loss : 0.2970, val_loss : 0.075769\n",
      "epoch : 18, train_loss : 0.2937, val_loss : 0.074449\n",
      "epoch : 19, train_loss : 0.2911, val_loss : 0.073246\n",
      "epoch : 20, train_loss : 0.2876, val_loss : 0.072944\n",
      "count: 1\n",
      "epoch : 21, train_loss : 0.2847, val_loss : 0.073554\n",
      "epoch : 22, train_loss : 0.2817, val_loss : 0.071261\n",
      "count: 1\n",
      "epoch : 23, train_loss : 0.2794, val_loss : 0.072155\n",
      "count: 2\n",
      "epoch : 24, train_loss : 0.2770, val_loss : 0.071468\n",
      "epoch : 25, train_loss : 0.2746, val_loss : 0.070490\n",
      "epoch : 26, train_loss : 0.2723, val_loss : 0.069749\n",
      "count: 1\n",
      "epoch : 27, train_loss : 0.2698, val_loss : 0.070591\n",
      "epoch : 28, train_loss : 0.2679, val_loss : 0.068962\n",
      "count: 1\n",
      "epoch : 29, train_loss : 0.2655, val_loss : 0.069191\n",
      "epoch : 30, train_loss : 0.2631, val_loss : 0.068212\n",
      "count: 1\n",
      "epoch : 31, train_loss : 0.2613, val_loss : 0.068903\n",
      "epoch : 32, train_loss : 0.2594, val_loss : 0.067845\n",
      "epoch : 33, train_loss : 0.2574, val_loss : 0.066462\n",
      "count: 1\n",
      "epoch : 34, train_loss : 0.2556, val_loss : 0.066689\n",
      "epoch : 35, train_loss : 0.2538, val_loss : 0.066338\n",
      "count: 1\n",
      "epoch : 36, train_loss : 0.2521, val_loss : 0.066454\n",
      "epoch : 37, train_loss : 0.2508, val_loss : 0.065164\n",
      "count: 1\n",
      "epoch : 38, train_loss : 0.2488, val_loss : 0.065315\n",
      "count: 2\n",
      "epoch : 39, train_loss : 0.2473, val_loss : 0.065180\n",
      "count: 3\n",
      "epoch : 40, train_loss : 0.2459, val_loss : 0.065807\n",
      "count: 4\n",
      "epoch : 41, train_loss : 0.2438, val_loss : 0.065210\n",
      "epoch : 42, train_loss : 0.2426, val_loss : 0.064822\n",
      "epoch : 43, train_loss : 0.2413, val_loss : 0.063940\n",
      "epoch : 44, train_loss : 0.2395, val_loss : 0.063845\n",
      "epoch : 45, train_loss : 0.2380, val_loss : 0.063662\n",
      "epoch : 46, train_loss : 0.2367, val_loss : 0.063384\n",
      "count: 1\n",
      "epoch : 47, train_loss : 0.2351, val_loss : 0.064584\n",
      "epoch : 48, train_loss : 0.2341, val_loss : 0.063139\n",
      "count: 1\n",
      "epoch : 49, train_loss : 0.2327, val_loss : 0.063271\n",
      "epoch : 50, train_loss : 0.2313, val_loss : 0.062388\n",
      "epoch : 51, train_loss : 0.2298, val_loss : 0.062013\n",
      "epoch : 52, train_loss : 0.2284, val_loss : 0.061853\n",
      "count: 1\n",
      "epoch : 53, train_loss : 0.2271, val_loss : 0.062263\n",
      "epoch : 54, train_loss : 0.2262, val_loss : 0.061844\n",
      "count: 1\n",
      "epoch : 55, train_loss : 0.2251, val_loss : 0.061905\n",
      "count: 2\n",
      "epoch : 56, train_loss : 0.2232, val_loss : 0.062039\n",
      "count: 3\n",
      "epoch : 57, train_loss : 0.2227, val_loss : 0.062724\n",
      "epoch : 58, train_loss : 0.2212, val_loss : 0.060891\n",
      "epoch : 59, train_loss : 0.2196, val_loss : 0.060500\n",
      "count: 1\n",
      "epoch : 60, train_loss : 0.2188, val_loss : 0.061128\n",
      "count: 2\n",
      "epoch : 61, train_loss : 0.2176, val_loss : 0.060648\n",
      "epoch : 62, train_loss : 0.2164, val_loss : 0.060424\n",
      "count: 1\n",
      "epoch : 63, train_loss : 0.2151, val_loss : 0.060642\n",
      "epoch : 64, train_loss : 0.2141, val_loss : 0.060291\n",
      "count: 1\n",
      "epoch : 65, train_loss : 0.2130, val_loss : 0.062129\n",
      "count: 2\n",
      "epoch : 66, train_loss : 0.2121, val_loss : 0.060488\n",
      "count: 3\n",
      "epoch : 67, train_loss : 0.2105, val_loss : 0.060297\n",
      "epoch : 68, train_loss : 0.2097, val_loss : 0.059127\n",
      "epoch : 69, train_loss : 0.2084, val_loss : 0.059124\n",
      "count: 1\n",
      "epoch : 70, train_loss : 0.2075, val_loss : 0.059337\n",
      "count: 2\n",
      "epoch : 71, train_loss : 0.2069, val_loss : 0.060791\n",
      "epoch : 72, train_loss : 0.2054, val_loss : 0.059013\n",
      "epoch : 73, train_loss : 0.2043, val_loss : 0.058440\n",
      "count: 1\n",
      "epoch : 74, train_loss : 0.2032, val_loss : 0.058759\n",
      "count: 2\n",
      "epoch : 75, train_loss : 0.2026, val_loss : 0.058637\n",
      "count: 3\n",
      "epoch : 76, train_loss : 0.2013, val_loss : 0.060185\n",
      "count: 4\n",
      "epoch : 77, train_loss : 0.2008, val_loss : 0.059901\n",
      "epoch : 78, train_loss : 0.1991, val_loss : 0.058355\n",
      "count: 1\n",
      "epoch : 79, train_loss : 0.1983, val_loss : 0.058478\n",
      "count: 2\n",
      "epoch : 80, train_loss : 0.1978, val_loss : 0.058647\n",
      "epoch : 81, train_loss : 0.1971, val_loss : 0.058022\n",
      "count: 1\n",
      "epoch : 82, train_loss : 0.1953, val_loss : 0.058171\n",
      "epoch : 83, train_loss : 0.1941, val_loss : 0.057530\n",
      "count: 1\n",
      "epoch : 84, train_loss : 0.1938, val_loss : 0.057584\n",
      "epoch : 85, train_loss : 0.1923, val_loss : 0.057504\n",
      "count: 1\n",
      "epoch : 86, train_loss : 0.1916, val_loss : 0.058030\n",
      "epoch : 87, train_loss : 0.1907, val_loss : 0.057168\n",
      "epoch : 88, train_loss : 0.1903, val_loss : 0.056816\n",
      "count: 1\n",
      "epoch : 89, train_loss : 0.1890, val_loss : 0.057296\n",
      "count: 2\n",
      "epoch : 90, train_loss : 0.1882, val_loss : 0.057022\n",
      "count: 3\n",
      "epoch : 91, train_loss : 0.1875, val_loss : 0.059500\n",
      "count: 4\n",
      "epoch : 92, train_loss : 0.1862, val_loss : 0.058567\n",
      "epoch : 93, train_loss : 0.1859, val_loss : 0.056731\n",
      "count: 1\n",
      "epoch : 94, train_loss : 0.1848, val_loss : 0.056761\n",
      "count: 2\n",
      "epoch : 95, train_loss : 0.1833, val_loss : 0.058354\n",
      "epoch : 96, train_loss : 0.1825, val_loss : 0.056515\n",
      "count: 1\n",
      "epoch : 97, train_loss : 0.1816, val_loss : 0.056996\n",
      "count: 2\n",
      "epoch : 98, train_loss : 0.1812, val_loss : 0.056870\n",
      "epoch : 99, train_loss : 0.1802, val_loss : 0.055851\n",
      "count: 1\n",
      "epoch : 100, train_loss : 0.1792, val_loss : 0.056541\n",
      "count: 2\n",
      "epoch : 101, train_loss : 0.1786, val_loss : 0.057016\n",
      "count: 3\n",
      "epoch : 102, train_loss : 0.1783, val_loss : 0.056981\n",
      "count: 4\n",
      "epoch : 103, train_loss : 0.1767, val_loss : 0.056331\n",
      "count: 5\n",
      "early stopping\n",
      "[1.04159045 0.54382491 0.46283203 0.42163554 0.39456424 0.37597391\n",
      " 0.36215913 0.35126755 0.3421689  0.33440951 0.32755804 0.32209167\n",
      " 0.31704152 0.31229922 0.3086758  0.30391279 0.30048016 0.29695946\n",
      " 0.29365149 0.29108489 0.2875925  0.28467616 0.28167367 0.27940282\n",
      " 0.27695659 0.27463475 0.27233461 0.26979169 0.26787478 0.26551002\n",
      " 0.26307431 0.26134774 0.25937006 0.25740784 0.25558135 0.25383499\n",
      " 0.25213474 0.2508423  0.2488071  0.24730061 0.24593011 0.24375199\n",
      " 0.24259004 0.24125484 0.23949771 0.23799986 0.23665881 0.2350888\n",
      " 0.23408301 0.232675   0.2313399  0.22982393 0.22837427 0.22708838\n",
      " 0.22620076 0.22506256 0.2231507  0.22266673 0.22116411 0.2195601\n",
      " 0.21878281 0.21761009 0.21642032 0.21505143 0.2140796  0.21302636\n",
      " 0.21214601 0.21045211 0.20971222 0.20844774 0.20752382 0.20690748\n",
      " 0.20535293 0.20429842 0.20315722 0.20256793 0.2013327  0.20075881\n",
      " 0.19913587 0.19828407 0.19776104 0.1970704  0.19534917 0.194058\n",
      " 0.19383827 0.19234733 0.19156584 0.19068386 0.19033054 0.18899328\n",
      " 0.18821834 0.18745524 0.18623154 0.18587701 0.18477298 0.18333629\n",
      " 0.1824788  0.18164043 0.18121701 0.18019955 0.17919031 0.17864206\n",
      " 0.17827554 0.17667814 0.17586531 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5),(0.5))])\n",
    "\n",
    "fashion_mnist_trainval = FashionMNIST(\"FashionMNIST\", train=True, download=True, transform=transform)\n",
    "fashion_mnist_test = FashionMNIST(\"FashionMNIST\", train=False, download=True, transform=transform)\n",
    "\n",
    "n_samples = len(fashion_mnist_trainval) \n",
    "train_size = int(len(fashion_mnist_trainval) * 0.8) \n",
    "val_size = n_samples - train_size \n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(fashion_mnist_trainval, [train_size, val_size])\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(fashion_mnist_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "w1 = torch.normal(0, 0.01, size=(28*28, 256), requires_grad=True)\n",
    "b1 = torch.zeros(256, requires_grad=True)\n",
    "w2 = torch.normal(0, 0.01, size=(256, classes), requires_grad=True)\n",
    "b2 = torch.zeros(classes, requires_grad=True)\n",
    "\n",
    "epoch_num = 1000\n",
    "optimizer = optim.SGD([w1, b1, w2, b2], lr = 0.02)\n",
    "running_loss = np.zeros(epoch_num)\n",
    "valrunning_loss = np.zeros(epoch_num)\n",
    "early_count = 0\n",
    "min_loss = 10\n",
    "delta = 0.001\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        images = images.view(-1,28*28)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = torch.matmul(images, w1)+ b1\n",
    "            outputs = Relu(outputs)\n",
    "            outputs = torch.matmul(outputs, w2)+ b2\n",
    "            pred_y = softmax(outputs)                 \n",
    "            loss = cross_entropy_loss(labels, pred_y) \n",
    "            running_loss[epoch] += torch.sum(loss)/batch_size\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    for i, (images, labels) in enumerate(val_loader): \n",
    "        images = images.view(-1,28*28)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = torch.matmul(images, w1)+ b1\n",
    "            outputs = Relu(outputs)\n",
    "            outputs = torch.matmul(outputs, w2)+ b2\n",
    "            pred_y = softmax(outputs)                 \n",
    "            val_loss = cross_entropy_loss(labels, pred_y) \n",
    "            valrunning_loss[epoch] += torch.sum(val_loss)/batch_size\n",
    "\n",
    "\n",
    "    if valrunning_loss[epoch] >= min_loss:\n",
    "        count += 1\n",
    "        print(\"count:\", count)\n",
    "    else:\n",
    "        count = 0\n",
    "        min_loss = valrunning_loss[epoch]\n",
    "        \n",
    "    if count >= 5:\n",
    "        print(\"early stopping\")\n",
    "        break\n",
    "        \n",
    "    print(\"epoch : %d, train_loss : %.4lf, val_loss : %4lf\" % (epoch, running_loss[epoch],valrunning_loss[epoch]))\n",
    "        \n",
    "        \n",
    "print(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc: 0.9112916666666667\n",
      "test_acc: 0.8737\n"
     ]
    }
   ],
   "source": [
    "train_acc = 0.0\n",
    "correct = 0.0\n",
    "count = 0.0\n",
    "\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    images = images.view(-1,28*28)\n",
    "    outputs = torch.matmul(images, w1)+ b1\n",
    "    outputs = Relu(outputs)\n",
    "    outputs = torch.matmul(outputs, w2)+ b2\n",
    "    pred_label1 = softmax(outputs)\n",
    "    pred_label = whichclass(pred_label1)\n",
    "    \n",
    "    for j in range(len(pred_label)):\n",
    "        if pred_label[j].int() == labels[j]:\n",
    "            correct += 1\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "train_acc = correct/count\n",
    "print(\"train_acc:\",train_acc)\n",
    "\n",
    "test_acc = 0.0\n",
    "correct = 0.0\n",
    "count = 0.0\n",
    "\n",
    "for i, (images, labels) in enumerate(test_loader):\n",
    "    images = images.view(-1,28*28)\n",
    "    outputs = torch.matmul(images, w1)+ b1\n",
    "    outputs = Relu(outputs)\n",
    "    outputs = torch.matmul(outputs, w2)+ b2\n",
    "    pred_label1 = softmax(outputs)\n",
    "    pred_label = whichclass(pred_label1)\n",
    "    \n",
    "    for j in range(len(pred_label)):\n",
    "        if pred_label[j].int() == labels[j]:\n",
    "            correct += 1\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "test_acc = correct/count\n",
    "print(\"test_acc:\",test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15dff6b2070>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(running_loss[0:epoch+1])\n",
    "plt.plot(valrunning_loss[0:epoch+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
