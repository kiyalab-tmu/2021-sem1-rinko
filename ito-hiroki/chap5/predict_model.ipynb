{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "executive-dinner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "\n",
    "from dataset import TimeMachineData, TimeMachineDataset\n",
    "from model import GRU, RNN, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "worst-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i : i + seq_len]\n",
    "    target = source[i + 1 : i + 1 + seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "published-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = f\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "temperature = 1.0\n",
    "bptt = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "superior-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "timemachine = TimeMachineData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "permanent-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = timemachine.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "creative-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_model = \"lstm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "attended-locking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embed): Embedding(94, 128)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (lstm): LSTM(128, 128, num_layers=2, dropout=0.3)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=94, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args_model == \"rnn\":\n",
    "    model = RNN(ntokens).to(device)\n",
    "elif args_model == \"gru\":\n",
    "    model = GRU(ntokens).to(device)\n",
    "elif args_model == \"lstm\":\n",
    "    model = LSTM(ntokens).to(device)\n",
    "else:\n",
    "    raise ValueError(\"Invalid model argument: {}\".format(args_model))\n",
    "model.load_state_dict(torch.load(f\"{args_model}_thetimemachine.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "standard-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path=\"../data/TheTimeMachine/35-0.txt\"\n",
    "dict_path=\"../data/TheTimeMachine/char.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "formed-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.read_csv(dict_path)\n",
    "char2idx = dict(zip(word_df[\"word\"], word_df.index))\n",
    "idx2char = dict(zip(word_df.index, word_df[\"word\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-tattoo",
   "metadata": {},
   "source": [
    "# multi char input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "measured-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A queer thing I soon\"\n",
    "text = [char2idx[\"<BOS>\"]] + [char2idx[char] for char in text.strip()]\n",
    "data = [text]\n",
    "data = torch.from_numpy(np.array(sum(data, []))).long()\n",
    "data = torch.unsqueeze(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "figured-trial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 31,  3, 51, 15,  4,  4, 11,  3,  5, 12,  9,  8, 19,  3, 26,  3, 10,\n",
       "          7,  7,  8]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "outdoor-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pharmaceutical-mouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, 31,  3, 51, 15,  4,  4, 11,  3,  5, 12,  9,  8, 19,  3, 26,  3, 10,\n",
       "          7,  7,  8]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afraid-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = model.init_hidden(data.shape[1])\n",
    "if args_model == \"lstm\":\n",
    "    cell = model.init_hidden(data.shape[1])\n",
    "result = data.cpu().numpy().tolist()[0]\n",
    "for i in range(100):\n",
    "    with torch.no_grad():\n",
    "        if args_model == \"lstm\":\n",
    "            output, hidden, cell = model(data, hidden, cell)\n",
    "        else:\n",
    "            output, hidden = model(data, hidden)\n",
    "        \n",
    "        word_weights = output.squeeze().data.div(1.0).exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1).squeeze()\n",
    "        \n",
    "        result += word_idx.cpu().numpy().tolist()\n",
    "        data = word_idx.reshape(1, -1)\n",
    "        \n",
    "        if 0 in word_idx.cpu().numpy().tolist():\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "focal-detective",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<BOS>A queer thing I soonPts(:8O_r”esitç z.lcd.rh;t”s h<EOS> islotu kt   o h ata tTlivh iœmdhtuIawbhltr lyeehb!aoern ieiedhaey  ras lnfedFv n  etf gmytimuk,n een<EOS>ats ewiy <EOS>sato ea  u<BOS>ze w—ag,a<BOS>iren'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([idx2char[x] for x in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-roulette",
   "metadata": {},
   "source": [
    "# single char input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "monthly-finance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>xond ay ay Wime son ferer this a<EOS>\n",
      "<BOS><EOS>\n",
      "<BOS>qlimin was a lait-et the wreow in the<EOS>\n",
      "<BOS>Xcone thauth to tituin,<EOS>\n",
      "<BOS>çenlofedWal<EOS>\n",
      "<BOS>flamkeadet clareudimly stust and sumile in speens ever thit had at<EOS>\n",
      "<BOS>conm burdy vemmess, and down ald in the breled the qapuinny To lent at all hamk<EOS>\n",
      "<BOS>-hopomot upot read me avibs in not I wharet that evind tremen I wand almy the Tinging, werled some I\n",
      "<BOS>heo<EOS>\n",
      "<BOS>bHemAcs Urfurdfised muce so, whe me<EOS>\n"
     ]
    }
   ],
   "source": [
    "for j in range(10):\n",
    "    text = [0]\n",
    "    data = [text]\n",
    "    data = torch.from_numpy(np.array(sum(data, []))).long()\n",
    "    data = torch.unsqueeze(data, 0)\n",
    "    data = data.to(device)\n",
    "    \n",
    "    hidden = model.init_hidden(1)\n",
    "    if args_model == \"lstm\":\n",
    "        cell = model.init_hidden(1)\n",
    "    result = [0]\n",
    "    for i in range(100):\n",
    "        if args_model == \"lstm\":\n",
    "            output, hidden, cell = model(data, hidden, cell)\n",
    "        else:\n",
    "            output, hidden = model(data, hidden)\n",
    "        \n",
    "        # word_idx = torch.argmax(output, dim=2).item()\n",
    "        word_weights = output.squeeze().data.div(1.0).exp().cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        \n",
    "        data.data.fill_(word_idx)\n",
    "        result.append(word_idx.item())\n",
    "        if word_idx == 1:\n",
    "            break\n",
    "    print(\"\".join([idx2char[x] for x in result]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-synthesis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
